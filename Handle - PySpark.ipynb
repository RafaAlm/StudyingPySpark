{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3686fd83-448a-462b-9e6d-8485d4e16684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951153c1-165e-43c6-bd9b-4a98588d2d96",
   "metadata": {},
   "source": [
    "Criar - Iniciar sessão PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9470893e-31fb-45d1-851a-20320575dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Pyspark01\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eecef88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('archive/wc2018-players.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e68b0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----+------------------+----------+----------+--------------------+------+------+\n",
      "|     Team|  #|Pos.| FIFA Popular Name|Birth Date|Shirt Name|                Club|Height|Weight|\n",
      "+---------+---+----+------------------+----------+----------+--------------------+------+------+\n",
      "|Argentina|  3|  DF|TAGLIAFICO Nicolas|31.08.1992|TAGLIAFICO|      AFC Ajax (NED)|   169|    65|\n",
      "|Argentina| 22|  MF|    PAVON Cristian|21.01.1996|     PAVÓN|CA Boca Juniors (...|   169|    65|\n",
      "|Argentina| 15|  MF|    LANZINI Manuel|15.02.1993|   LANZINI|West Ham United F...|   167|    66|\n",
      "|Argentina| 18|  DF|    SALVIO Eduardo|13.07.1990|    SALVIO|    SL Benfica (POR)|   167|    69|\n",
      "|Argentina| 10|  FW|      MESSI Lionel|24.06.1987|     MESSI|  FC Barcelona (ESP)|   170|    72|\n",
      "+---------+---+----+------------------+----------+----------+--------------------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c62b0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Team: string (nullable = true)\n",
      " |-- #: integer (nullable = true)\n",
      " |-- Pos.: string (nullable = true)\n",
      " |-- FIFA Popular Name: string (nullable = true)\n",
      " |-- Birth Date: string (nullable = true)\n",
      " |-- Shirt Name: string (nullable = true)\n",
      " |-- Club: string (nullable = true)\n",
      " |-- Height: integer (nullable = true)\n",
      " |-- Weight: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa28e4",
   "metadata": {},
   "source": [
    "Verificar valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee5272b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Team                 0\n",
       "#                    0\n",
       "Pos.                 0\n",
       "FIFA Popular Name    0\n",
       "Birth Date           0\n",
       "Shirt Name           0\n",
       "Club                 0\n",
       "Height               0\n",
       "Weight               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas().isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78be27aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+------------------+----------+----------+--------------------+------+----+\n",
      "|  Selecao|Numero|Posicao|              Nome|Nascimento|Shirt Name|                Time|Altura|Peso|\n",
      "+---------+------+-------+------------------+----------+----------+--------------------+------+----+\n",
      "|Argentina|     3|     DF|TAGLIAFICO Nicolas|31.08.1992|TAGLIAFICO|      AFC Ajax (NED)|   169|  65|\n",
      "|Argentina|    22|     MF|    PAVON Cristian|21.01.1996|     PAVÓN|CA Boca Juniors (...|   169|  65|\n",
      "|Argentina|    15|     MF|    LANZINI Manuel|15.02.1993|   LANZINI|West Ham United F...|   167|  66|\n",
      "|Argentina|    18|     DF|    SALVIO Eduardo|13.07.1990|    SALVIO|    SL Benfica (POR)|   167|  69|\n",
      "|Argentina|    10|     FW|      MESSI Lionel|24.06.1987|     MESSI|  FC Barcelona (ESP)|   170|  72|\n",
      "+---------+------+-------+------------------+----------+----------+--------------------+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('Team', 'Selecao').withColumnRenamed('#', 'Numero').withColumnRenamed('Pos.', 'Posicao')\\\n",
    ".withColumnRenamed('FIFA Popular Name', 'Nome').withColumnRenamed('Birth Date', 'Nascimento')\\\n",
    ".withColumnRenamed('Club', 'Time').withColumnRenamed('Height', 'Altura').withColumnRenamed('Weight', 'Peso')\\\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0632295a",
   "metadata": {},
   "source": [
    "Verificar valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a7c7629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecao 0\n",
      "Numero 0\n",
      "Posicao 0\n",
      "Nome 0\n",
      "Nascimento 0\n",
      "Shirt Name 0\n",
      "Time 0\n",
      "Altura 0\n",
      "Peso 0\n"
     ]
    }
   ],
   "source": [
    "for coluna in df.columns:\n",
    "    print(coluna, df.filter(df[coluna].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1da27eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selecao',\n",
       " 'Numero',\n",
       " 'Posicao',\n",
       " 'Nome',\n",
       " 'Nascimento',\n",
       " 'Shirt Name',\n",
       " 'Time',\n",
       " 'Altura',\n",
       " 'Peso']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8abcf17",
   "metadata": {},
   "source": [
    "Selecionar colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d157b927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|              Nome|  Selecao|\n",
      "+------------------+---------+\n",
      "|TAGLIAFICO Nicolas|Argentina|\n",
      "|    PAVON Cristian|Argentina|\n",
      "|    LANZINI Manuel|Argentina|\n",
      "|    SALVIO Eduardo|Argentina|\n",
      "|      MESSI Lionel|Argentina|\n",
      "|  ANSALDI Cristian|Argentina|\n",
      "|      BIGLIA Lucas|Argentina|\n",
      "+------------------+---------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Nome','Selecao').show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "918d1ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Time|\n",
      "+--------------------+\n",
      "|      AFC Ajax (NED)|\n",
      "|CA Boca Juniors (...|\n",
      "|West Ham United F...|\n",
      "|    SL Benfica (POR)|\n",
      "|  FC Barcelona (ESP)|\n",
      "|     Torino FC (ITA)|\n",
      "|      AC Milan (ITA)|\n",
      "+--------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('Time')).show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c873289b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|  Selecao|\n",
      "+---------+\n",
      "|Argentina|\n",
      "|Argentina|\n",
      "|Argentina|\n",
      "+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['Selecao']).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d6de97",
   "metadata": {},
   "source": [
    "Selecionar coluna com ALIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edcbaaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     Time|\n",
      "+---------+\n",
      "|Argentina|\n",
      "|Argentina|\n",
      "|Argentina|\n",
      "|Argentina|\n",
      "|Argentina|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('Selecao').alias('Time')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae30f89",
   "metadata": {},
   "source": [
    "Select utilizando split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54745aa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------+\n",
      "|  Selecao|              nome|altura|\n",
      "+---------+------------------+------+\n",
      "|Argentina|TAGLIAFICO Nicolas|   169|\n",
      "|Argentina|    PAVON Cristian|   169|\n",
      "|Argentina|    LANZINI Manuel|   167|\n",
      "|Argentina|    SALVIO Eduardo|   167|\n",
      "|Argentina|      MESSI Lionel|   170|\n",
      "+---------+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Selecao nome altura'.split()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61746d",
   "metadata": {},
   "source": [
    "Filtrar DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8de5ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-----------+----------+-----------+--------------------+------+----+\n",
      "|Selecao|Numero|Posicao|       Nome|Nascimento| Shirt Name|                Time|Altura|Peso|\n",
      "+-------+------+-------+-----------+----------+-----------+--------------------+------+----+\n",
      "| Brazil|    18|     MF|       FRED|05.03.1993|       FRED|FC Shakhtar Donet...|   169|  64|\n",
      "| Brazil|    21|     FW|     TAISON|13.01.1988|     TAISON|FC Shakhtar Donet...|   172|  64|\n",
      "| Brazil|    17|     MF|FERNANDINHO|04.05.1985|FERNANDINHO|Manchester City F...|   179|  67|\n",
      "+-------+------+-------+-----------+----------+-----------+--------------------+------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('Selecao = \"Brazil\"').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede8b1f0",
   "metadata": {},
   "source": [
    "Filtrar com mais de uma condição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d4ab89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+----+----------+----------+--------------------+------+----+\n",
      "|Selecao|Numero|Posicao|Nome|Nascimento|Shirt Name|                Time|Altura|Peso|\n",
      "+-------+------+-------+----+----------+----------+--------------------+------+----+\n",
      "| Brazil|    18|     MF|FRED|05.03.1993|      FRED|FC Shakhtar Donet...|   169|  64|\n",
      "+-------+------+-------+----+----------+----------+--------------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((col('Selecao') == \"Brazil\") & (col('Altura') == 169)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ae2bc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+-----------------+----------+-----------+--------------------+------+----+\n",
      "|Selecao|Numero|Posicao|             Nome|Nascimento| Shirt Name|                Time|Altura|Peso|\n",
      "+-------+------+-------+-----------------+----------+-----------+--------------------+------+----+\n",
      "| Brazil|    18|     MF|             FRED|05.03.1993|       FRED|FC Shakhtar Donet...|   169|  64|\n",
      "| Brazil|    21|     FW|           TAISON|13.01.1988|     TAISON|FC Shakhtar Donet...|   172|  64|\n",
      "| Brazil|    17|     MF|      FERNANDINHO|04.05.1985|FERNANDINHO|Manchester City F...|   179|  67|\n",
      "| Brazil|    22|     DF|           FAGNER|11.06.1989|     FAGNER|SC Corinthians (BRA)|   168|  67|\n",
      "| Brazil|    11|     MF|PHILIPPE COUTINHO|12.06.1992|P. COUTINHO|  FC Barcelona (ESP)|   172|  68|\n",
      "+-------+------+-------+-----------------+----------+-----------+--------------------+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('Selecao = \"Brazil\"').filter(col('Numero') > 10).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71af9c2",
   "metadata": {},
   "source": [
    "Filtro condicional \"OU\" = |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35a585b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+------------+----------+----------+------------------+------+----+\n",
      "|  Selecao|Numero|Posicao|        Nome|Nascimento|Shirt Name|              Time|Altura|Peso|\n",
      "+---------+------+-------+------------+----------+----------+------------------+------+----+\n",
      "|Argentina|    10|     FW|MESSI Lionel|24.06.1987|     MESSI|FC Barcelona (ESP)|   170|  72|\n",
      "+---------+------+-------+------------+----------+----------+------------------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((col('Nome') == 'MESSI Lionel') | (col('Nome') == 'Fred')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7167c6",
   "metadata": {},
   "source": [
    "Filtrar DF combinando & e | (and - Or)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ac5eef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+------------+----------+-----------+--------------------+------+----+\n",
      "|  Selecao|Numero|Posicao|        Nome|Nascimento| Shirt Name|                Time|Altura|Peso|\n",
      "+---------+------+-------+------------+----------+-----------+--------------------+------+----+\n",
      "|Argentina|    10|     FW|MESSI Lionel|24.06.1987|      MESSI|  FC Barcelona (ESP)|   170|  72|\n",
      "|   Brazil|    22|     DF|      FAGNER|11.06.1989|     FAGNER|SC Corinthians (BRA)|   168|  67|\n",
      "|   Brazil|     6|     DF| FILIPE LUIS|09.08.1985|FILIPE LUIS|Atletico Madrid (...|   182|  73|\n",
      "|   Brazil|    13|     DF|  MARQUINHOS|14.05.1994| MARQUINHOS|Paris Saint-Germa...|   183|  75|\n",
      "|   Brazil|     3|     DF|     MIRANDA|07.09.1984|    MIRANDA|FC Internazionale...|   186|  78|\n",
      "+---------+------+-------+------------+----------+-----------+--------------------+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((col('Selecao') == \"Brazil\") & (col('Posicao') == 'DF') | (col('Nome') == 'MESSI Lionel')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fabc877d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+------------------+----------+----------+--------------------+------+----+\n",
      "|  Selecao|Numero|Posicao|              Nome|Nascimento|Shirt Name|                Time|Altura|Peso|\n",
      "+---------+------+-------+------------------+----------+----------+--------------------+------+----+\n",
      "|Argentina|     3|     DF|TAGLIAFICO Nicolas|31.08.1992|TAGLIAFICO|      AFC Ajax (NED)|   169|  65|\n",
      "|Argentina|    22|     MF|    PAVON Cristian|21.01.1996|     PAVÓN|CA Boca Juniors (...|   169|  65|\n",
      "|Argentina|    15|     MF|    LANZINI Manuel|15.02.1993|   LANZINI|West Ham United F...|   167|  66|\n",
      "+---------+------+-------+------------------+----------+----------+--------------------+------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16ed8a1",
   "metadata": {},
   "source": [
    "Criar uma coluna nova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55625fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Ano', substring('Nascimento', -4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46d10edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+------------------+----------+----------+--------------------+------+----+----+\n",
      "|  Selecao|Numero|Posicao|              Nome|Nascimento|Shirt Name|                Time|Altura|Peso| Ano|\n",
      "+---------+------+-------+------------------+----------+----------+--------------------+------+----+----+\n",
      "|Argentina|     3|     DF|TAGLIAFICO Nicolas|31.08.1992|TAGLIAFICO|      AFC Ajax (NED)|   169|  65|1992|\n",
      "|Argentina|    22|     MF|    PAVON Cristian|21.01.1996|     PAVÓN|CA Boca Juniors (...|   169|  65|1996|\n",
      "|Argentina|    15|     MF|    LANZINI Manuel|15.02.1993|   LANZINI|West Ham United F...|   167|  66|1993|\n",
      "|Argentina|    18|     DF|    SALVIO Eduardo|13.07.1990|    SALVIO|    SL Benfica (POR)|   167|  69|1990|\n",
      "|Argentina|    10|     FW|      MESSI Lionel|24.06.1987|     MESSI|  FC Barcelona (ESP)|   170|  72|1987|\n",
      "|Argentina|     4|     DF|  ANSALDI Cristian|20.09.1986|   ANSALDI|     Torino FC (ITA)|   181|  73|1986|\n",
      "|Argentina|     5|     MF|      BIGLIA Lucas|30.01.1986|    BIGLIA|      AC Milan (ITA)|   175|  73|1986|\n",
      "|Argentina|     7|     MF|       BANEGA Ever|29.06.1988|    BANEGA|    Sevilla FC (ESP)|   175|  73|1988|\n",
      "|Argentina|    14|     DF| MASCHERANO Javier|08.06.1984|MASCHERANO|Hebei China Fortu...|   174|  73|1984|\n",
      "|Argentina|    21|     FW|      DYBALA Paulo|15.11.1993|    DYBALA|   Juventus FC (ITA)|   177|  73|1993|\n",
      "|Argentina|    19|     FW|     AGUERO Sergio|02.06.1988|    AGÜERO|Manchester City F...|   172|  74|1988|\n",
      "|Argentina|     9|     FW|   HIGUAIN Gonzalo|10.12.1987|   HIGUAÍN|   Juventus FC (ITA)|   184|  75|1987|\n",
      "|Argentina|    11|     MF|    DI MARIA Angel|14.02.1988|  DI MARÍA|Paris Saint-Germa...|   178|  75|1988|\n",
      "|Argentina|    20|     MF|  LO CELSO Giovani|09.04.1996|  LO CELSO|Paris Saint-Germa...|   177|  75|1996|\n",
      "|Argentina|    13|     MF|  MEZA Maximiliano|15.12.1992|      MEZA|CA Independiente ...|   180|  76|1992|\n",
      "|Argentina|     8|     DF|      ACUNA Marcos|28.10.1991|     ACUÑA|   Sporting CP (POR)|   172|  77|1991|\n",
      "|Argentina|    23|     GK|CABALLERO Wilfredo|28.09.1981| CABALLERO|    Chelsea FC (ENG)|   186|  80|1981|\n",
      "|Argentina|     2|     DF|   MERCADO Gabriel|18.03.1987|   MERCADO|    Sevilla FC (ESP)|   181|  81|1987|\n",
      "|Argentina|    17|     DF|  OTAMENDI Nicolas|12.02.1988|  OTAMENDI|Manchester City F...|   181|  81|1988|\n",
      "|Argentina|    16|     DF|       ROJO Marcos|20.03.1990|      ROJO|Manchester United...|   189|  82|1990|\n",
      "+---------+------+-------+------------------+----------+----------+--------------------+------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85580a5d",
   "metadata": {},
   "source": [
    "Concatenar uma coluna com a outra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6049aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+------------------+----------+----------+--------------------+------+----+----+--------------+\n",
      "|  Selecao|Numero|Posicao|              Nome|Nascimento|Shirt Name|                Time|Altura|Peso| Ano|Selecao_numero|\n",
      "+---------+------+-------+------------------+----------+----------+--------------------+------+----+----+--------------+\n",
      "|Argentina|     3|     DF|TAGLIAFICO Nicolas|31.08.1992|TAGLIAFICO|      AFC Ajax (NED)|   169|  65|1992|    Argentina3|\n",
      "|Argentina|    22|     MF|    PAVON Cristian|21.01.1996|     PAVÓN|CA Boca Juniors (...|   169|  65|1996|   Argentina22|\n",
      "|Argentina|    15|     MF|    LANZINI Manuel|15.02.1993|   LANZINI|West Ham United F...|   167|  66|1993|   Argentina15|\n",
      "+---------+------+-------+------------------+----------+----------+--------------------+------+----+----+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Selecao_numero', concat('Selecao', col('Numero'))).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab34d8",
   "metadata": {},
   "source": [
    "Concatenar uma coluna utilizando separador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e15710fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-------+------------------+----------+----------+--------------------+------+----+----+--------------+\n",
      "|  Selecao|Numero|Posicao|              Nome|Nascimento|Shirt Name|                Time|Altura|Peso| Ano|Selecao_numero|\n",
      "+---------+------+-------+------------------+----------+----------+--------------------+------+----+----+--------------+\n",
      "|Argentina|     3|     DF|TAGLIAFICO Nicolas|31.08.1992|TAGLIAFICO|      AFC Ajax (NED)|   169|  65|1992| Argentina - 3|\n",
      "|Argentina|    22|     MF|    PAVON Cristian|21.01.1996|     PAVÓN|CA Boca Juniors (...|   169|  65|1996|Argentina - 22|\n",
      "|Argentina|    15|     MF|    LANZINI Manuel|15.02.1993|   LANZINI|West Ham United F...|   167|  66|1993|Argentina - 15|\n",
      "+---------+------+-------+------------------+----------+----------+--------------------+------+----+----+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Selecao_numero', concat_ws(' - ', 'Selecao', 'Numero')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3613a56",
   "metadata": {},
   "source": [
    "Alterar o tipo da coluna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2f390c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Selecao: string (nullable = true)\n",
      " |-- Numero: integer (nullable = true)\n",
      " |-- Posicao: string (nullable = true)\n",
      " |-- Nome: string (nullable = true)\n",
      " |-- Nascimento: string (nullable = true)\n",
      " |-- Shirt Name: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Altura: integer (nullable = true)\n",
      " |-- Peso: integer (nullable = true)\n",
      " |-- Ano: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93302bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = df.withColumn('Ano', col('Ano').cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6dda7477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Selecao: string (nullable = true)\n",
      " |-- Numero: integer (nullable = true)\n",
      " |-- Posicao: string (nullable = true)\n",
      " |-- Nome: string (nullable = true)\n",
      " |-- Nascimento: string (nullable = true)\n",
      " |-- Shirt Name: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Altura: integer (nullable = true)\n",
      " |-- Peso: integer (nullable = true)\n",
      " |-- Ano: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22e97a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dia = udf(lambda data: data.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64a0b957",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o231.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 51.0 failed 1 times, most recent failure: Lost task 0.0 in stage 51.0 (TID 42) (KMS-NB258 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 25 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDia\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdia\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNascimento\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o231.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 51.0 failed 1 times, most recent failure: Lost task 0.0 in stage 51.0 (TID 42) (KMS-NB258 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 25 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 25 more\r\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Dia', dia('Nascimento')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62847cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430cc000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d06971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ade86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf4dcf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e705f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8d7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53c3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
